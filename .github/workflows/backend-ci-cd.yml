name: Backend CI/CD to Google Cloud Run

on:
  push:
    branches: [ main ]
    paths:
      - 'BE/**'
      - '.github/workflows/**' # Trigger on workflow changes too
  pull_request:
    branches: [ main ]
    paths:
      - 'BE/**'
  workflow_dispatch:

env:
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCP_REGION: ${{ secrets.GCP_REGION }} # e.g., us-central1
  CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
  DB_USER: ${{ secrets.DB_USER }}
  DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
  DB_NAME: ${{ secrets.DB_NAME }}
  SERVICE_NAME: my-backend-service # Choose a name for your Cloud Run service
  IMAGE_REPO_NAME: my-backend-repo # Name of your Artifact Registry repo
  DOCKERFILE_PATH: ./BE/Dockerfile # Path to your Dockerfile relative to repo root
  WORKING_DIRECTORY: ./BE # Directory where your backend code and requirements.txt reside

jobs:
  build-lint-test:
    runs-on: ubuntu-latest
    # This job will use a service container for DB for tests, not Cloud SQL
    services:
      postgres_test: # Renamed to avoid confusion with prod
        image: postgres:15
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.11'
          cache: 'pip'
          cache-dependency-path: ${{ env.WORKING_DIRECTORY }}/requirements.txt

      - name: Install dependencies
        working-directory: ${{ env.WORKING_DIRECTORY }}
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest psycopg2-binary alembic # Ensure test/migration tools are here
          pip install -r requirements.txt

      - name: Lint with flake8
        working-directory: ${{ env.WORKING_DIRECTORY }}
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      # - name: Test with pytest
      #   working-directory: ${{ env.WORKING_DIRECTORY }}
      #   run: |
      #     pytest tests/ # Assuming tests are in BE/tests/
      #   env:
      #     DATABASE_URL: postgresql://testuser:testpassword@localhost:5432/testdb # Connect to service DB
      #     # Pass other necessary secrets for tests
      #     GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      #     OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  deploy-to-cloud-run:
    needs: build-lint-test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      # Set up Python for migration step (NEEDS TO BE ADDED)
      - name: Set up Python for migrations
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.11' # Match your project's Python version
          cache: 'pip' # Optional: cache pip dependencies
          # If your alembic setup depends on your main requirements.txt, 
          # you might want to specify the cache dependency path for that specific working directory too.
          # cache-dependency-path: ${{ env.WORKING_DIRECTORY }}/requirements.txt

      # Install Alembic and DB driver for migrations (NEEDS TO BE ADDED)
      - name: Install Alembic and DB driver
        working-directory: ${{ env.WORKING_DIRECTORY }} # Ensure this is where your requirements.txt for BE is
        run: |
          python -m pip install --upgrade pip
          # Install specific packages needed for alembic to run
          # psycopg2-binary for PostgreSQL, alembic itself.
          # If your migrations/env.py imports models directly from your app,
          # you might need to install your full requirements.txt
          pip install alembic psycopg2-binary sqlalchemy
          # Or, if your migrations/env.py is complex and imports many parts of your app:
          # pip install -r requirements.txt

      - name: Configure Docker
        run: gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev --quiet

      - name: Build and Push Docker image to Artifact Registry
        env:
          IMAGE_TAG: ${{ github.sha }}
          IMAGE_NAME: ${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.IMAGE_REPO_NAME }}/${{ env.SERVICE_NAME }}
        run: |
          echo "Building image: $IMAGE_NAME:$IMAGE_TAG from Dockerfile: ${{ env.DOCKERFILE_PATH }} with context: ${{ env.WORKING_DIRECTORY }}"
          docker build -t "$IMAGE_NAME:$IMAGE_TAG" -f ${{ env.DOCKERFILE_PATH }} ${{ env.WORKING_DIRECTORY }}
          docker push "$IMAGE_NAME:$IMAGE_TAG"
          docker tag "$IMAGE_NAME:$IMAGE_TAG" "$IMAGE_NAME:latest"
          docker push "$IMAGE_NAME:latest"

      - name: Deploy to Cloud Run
        env:
          IMAGE_TO_DEPLOY: ${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.IMAGE_REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }}
          PROD_DATABASE_URL: "postgresql+psycopg2://${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@/${{ env.DB_NAME }}?host=/cloudsql/${{ env.CLOUD_SQL_CONNECTION_NAME }}"
          #GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }} # <--- ADDED for GCS
        run: |
          gcloud run deploy ${{ env.SERVICE_NAME }} \
            --image ${{ env.IMAGE_TO_DEPLOY }} \
            --platform managed \
            --region ${{ env.GCP_REGION }} \
            --allow-unauthenticated \
            --port 8080 \
            --add-cloudsql-instances ${{ env.CLOUD_SQL_CONNECTION_NAME }} \
            --set-env-vars "DATABASE_URL=${PROD_DATABASE_URL}" \
            --set-env-vars "GOOGLE_API_KEY=${{ secrets.GOOGLE_API_KEY }}" \
            --set-env-vars "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" \
            --project ${{ env.GCP_PROJECT_ID }} \
            --quiet

      - name: Install Cloud SQL Auth Proxy
        # This step downloads the proxy to the root of the runner's workspace for this job
        run: |
          wget https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.8.2/cloud-sql-proxy.linux.amd64 -O cloud-sql-proxy
          chmod +x cloud-sql-proxy

      - name: Run Database Migrations via Cloud SQL Proxy
        working-directory: ${{ env.WORKING_DIRECTORY }} # alembic.ini should be here or in a subdir like 'migrations'
        env:
          DATABASE_URL: "postgresql+psycopg2://${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@127.0.0.1:5432/${{ env.DB_NAME }}"
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          #GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }} # Ensure alembic env.py can access if needed
        run: |
          # The proxy was downloaded to the GITHUB_WORKSPACE root, so reference it from there
          # Or, ensure the "Install Cloud SQL Auth Proxy" step has a working-directory set
          # so that ./cloud-sql-proxy is relative to that.
          # Easiest: use absolute path from GITHUB_WORKSPACE
          ${{ github.workspace }}/cloud-sql-proxy --quiet --credentials-file "$GOOGLE_APPLICATION_CREDENTIALS" ${{ env.CLOUD_SQL_CONNECTION_NAME }} &
          PROXY_PID=$! # Capture the PID of the background process
          echo "Cloud SQL Proxy PID: $PROXY_PID"
          sleep 5 # Give proxy time to start

          # Check if proxy is running
          if ! ps -p $PROXY_PID > /dev/null; then
            echo "Cloud SQL Proxy failed to start or exited prematurely."
            # You can try to get logs from proxy if it has an option, or check dmesg if possible
            exit 1
          fi
          
          echo "Running Alembic migrations..."
          alembic upgrade head

          echo "Killing Cloud SQL Proxy (PID: $PROXY_PID)..."
          kill $PROXY_PID
          # Wait for the proxy to actually terminate. Add a timeout to prevent hanging.
          # The 'timeout' command might not be available on all runners, 'wait' is more portable.
          # If 'wait $PROXY_PID' hangs, the trap below will eventually kill it.
          # Using a more robust kill and wait:
          if ps -p $PROXY_PID > /dev/null; then # Check if still running before trying to kill
             kill $PROXY_PID
             for i in {1..5}; do # Wait up to 5 seconds
                 if ! ps -p $PROXY_PID > /dev/null; then
                     break
                 fi
                 sleep 1
             done
             if ps -p $PROXY_PID > /dev/null; then # If still running, force kill
                 echo "Proxy did not terminate gracefully, force killing..."
                 kill -9 $PROXY_PID
             fi
          fi
          echo "Cloud SQL Proxy should be stopped."
