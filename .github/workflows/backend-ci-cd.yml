name: Backend CI/CD to Google Cloud Run

on:
  push:
    branches: [ main ]
    paths:
      - 'BE/**'
      - '.github/workflows/**' # Trigger on workflow changes too
  pull_request:
    branches: [ main ]
    paths:
      - 'BE/**'
  workflow_dispatch:

env:
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCP_REGION: ${{ secrets.GCP_REGION }} # e.g., us-central1
  CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
  DB_USER: ${{ secrets.DB_USER }}
  DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
  DB_NAME: ${{ secrets.DB_NAME }}
  SERVICE_NAME: my-backend-service # Choose a name for your Cloud Run service
  IMAGE_REPO_NAME: my-backend-repo # Name of your Artifact Registry repo
  DOCKERFILE_PATH: ./BE/Dockerfile # Path to your Dockerfile relative to repo root
  WORKING_DIRECTORY: ./BE # Directory where your backend code and requirements.txt reside

jobs:
  build-lint-test:
    runs-on: ubuntu-latest
    # This job will use a service container for DB for tests, not Cloud SQL
    services:
      postgres_test: # Renamed to avoid confusion with prod
        image: postgres:15
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.11'
          cache: 'pip'
          cache-dependency-path: ${{ env.WORKING_DIRECTORY }}/requirements.txt

      - name: Install dependencies
        working-directory: ${{ env.WORKING_DIRECTORY }}
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest psycopg2-binary alembic # Ensure test/migration tools are here
          pip install -r requirements.txt

      - name: Lint with flake8
        working-directory: ${{ env.WORKING_DIRECTORY }}
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Test with pytest
        working-directory: ${{ env.WORKING_DIRECTORY }}
        run: |
          pytest tests/ # Assuming tests are in BE/tests/
        env:
          DATABASE_URL: postgresql://testuser:testpassword@localhost:5432/testdb # Connect to service DB
          # Pass other necessary secrets for tests
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  deploy-to-cloud-run:
    needs: build-lint-test # Ensure tests pass before deploying
    runs-on: ubuntu-latest
    # Only run on push to main branch
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    # Add 'id-token' with 'write' permission for Workload Identity Federation
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          # For Workload Identity Federation (more secure, preferred but more setup):
          # workload_identity_provider: 'projects/${{ env.GCP_PROJECT_ID }}/locations/global/workloadIdentityPools/YOUR_POOL_ID/providers/YOUR_PROVIDER_ID'
          # service_account: 'github-actions-deployer@${{ env.GCP_PROJECT_ID }}.iam.gserviceaccount.com'

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      # Configure Docker to use gcloud credential helper
      - name: Configure Docker
        run: gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev --quiet

      - name: Build and Push Docker image to Artifact Registry
        env:
          IMAGE_TAG: ${{ github.sha }}
          IMAGE_NAME: ${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.IMAGE_REPO_NAME }}/${{ env.SERVICE_NAME }}
        run: |
          echo "Building image: $IMAGE_NAME:$IMAGE_TAG from Dockerfile: ${{ env.DOCKERFILE_PATH }} with context: ${{ env.WORKING_DIRECTORY }}"
          docker build -t "$IMAGE_NAME:$IMAGE_TAG" -f ${{ env.DOCKERFILE_PATH }} ${{ env.WORKING_DIRECTORY }}
          docker push "$IMAGE_NAME:$IMAGE_TAG"
          docker tag "$IMAGE_NAME:$IMAGE_TAG" "$IMAGE_NAME:latest" # Optionally tag as latest
          docker push "$IMAGE_NAME:latest"

      - name: Deploy to Cloud Run
        env:
          IMAGE_TO_DEPLOY: ${{ env.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.IMAGE_REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }}
          # Construct DATABASE_URL for Cloud Run environment
          # Cloud Run will connect to Cloud SQL over private IP. The host is the private IP.
          # We need to discover the private IP or use the instance connection name with the proxy.
          # Cloud Run's --add-cloudsql-instances handles the proxy setup.
          # So, the connection string for psycopg2 should use the socket path for the proxy.
          # Format: postgresql+psycopg2://USER:PASSWORD@/DATABASE?host=/cloudsql/INSTANCE_CONNECTION_NAME
          PROD_DATABASE_URL: "postgresql+psycopg2://${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@/${{ env.DB_NAME }}?host=/cloudsql/${{ env.CLOUD_SQL_CONNECTION_NAME }}"
        run: |
          gcloud run deploy ${{ env.SERVICE_NAME }} \
            --image ${{ env.IMAGE_TO_DEPLOY }} \
            --platform managed \
            --region ${{ env.GCP_REGION }} \
            --allow-unauthenticated \
            --port 8080 \
            --add-cloudsql-instances ${{ env.CLOUD_SQL_CONNECTION_NAME }} \
            --set-env-vars "DATABASE_URL=${PROD_DATABASE_URL}" \
            --set-env-vars "GOOGLE_API_KEY=${{ secrets.GOOGLE_API_KEY }}" \
            --set-env-vars "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" \
            --project ${{ env.GCP_PROJECT_ID }} \
            --quiet

      # Optional: Run Database Migrations after successful deployment
      # This step is more complex as it needs to run commands against the deployed service
      # or have a way to connect to Cloud SQL from the runner securely.
      # One way is to invoke a command on the new Cloud Run revision, or use a Cloud Run Job.
      # For simplicity here, we'll attempt to run alembic using gcloud exec if your app includes alembic CLI.
      # A more robust way is to have a dedicated migration script or command in your Docker image.
      # This example assumes your Docker image (and thus Cloud Run) has alembic and can run migrations.
      # For this to work, the deployed Cloud Run service needs alembic and its configuration.
      # The DATABASE_URL env var set above will be used by alembic.
      # This step is EXPERIMENTAL for direct gcloud run exec. A Cloud Run Job is better.
      #
      # - name: Run Database Migrations on Cloud Run (Requires 'gcloud beta run services update --execute-command')
      #   if: success() # Only run if deployment was successful
      #   env:
      #     # The DATABASE_URL is already set in the Cloud Run service environment
      #     PROD_DATABASE_URL: "postgresql+psycopg2://${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@/${{ env.DB_NAME }}?host=/cloudsql/${{ env.CLOUD_SQL_CONNECTION_NAME }}"
      #   run: |
      #     echo "Attempting to run migrations. This feature is evolving."
      #     # Note: Cloud Run `execute-command` is for interactive shells.
      #     # For migrations, a better approach is:
      #     # 1. App runs migrations on startup (carefully, with leader election if multiple instances)
      #     # 2. A separate Cloud Run Job for migrations.
      #     # 3. A custom entrypoint in your Docker image that runs migrations then starts the app.
      #     #
      #     # If your Docker CMD is `gunicorn ...`, you can't just `alembic upgrade head`.
      #     # You would need a separate entrypoint script or a different command.
      #     # For now, let's assume you'll run migrations manually or via a Cloud Run Job.
      #     echo "INFO: Manual migration or Cloud Run Job recommended for database migrations."
      #     echo "Example: gcloud beta run jobs create migration-job --image ... --command alembic --args upgrade,head ..."
      #     echo "Then: gcloud beta run jobs execute migration-job"
      #
      # Simpler approach for now: Run migrations from the CI runner using Cloud SQL Auth Proxy
      # This requires Cloud SQL Auth Proxy to be installed on the runner.
      - name: Install Cloud SQL Auth Proxy
        run: |
          wget https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.8.2/cloud-sql-proxy.linux.amd64 -O cloud-sql-proxy
          chmod +x cloud-sql-proxy

      - name: Run Database Migrations via Cloud SQL Proxy
        working-directory: ${{ env.WORKING_DIRECTORY }} # Where alembic.ini is
        env:
          # This DATABASE_URL connects to the proxy, which then connects to Cloud SQL
          DATABASE_URL: "postgresql+psycopg2://${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@127.0.0.1:5432/${{ env.DB_NAME }}"
          # Other env vars needed by alembic's env.py if it loads full app config
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          ./cloud-sql-proxy --quiet --credentials-file "$GOOGLE_APPLICATION_CREDENTIALS" ${{ env.CLOUD_SQL_CONNECTION_NAME }} & # Run proxy in background
          sleep 5 # Give proxy time to start
          alembic upgrade head
          # Kill the proxy, ensure it's killed even if alembic fails
          PROXY_PID=$(pgrep cloud-sql-proxy)
          if [ -n "$PROXY_PID" ]; then
            kill $PROXY_PID
            wait $PROXY_PID || true # Wait for it to exit, ignore error if already exited
          fi
